{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8134871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入相关包\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "#创建模型 -待完成\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,SimpleRNN\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adam\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f708be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#函数预处理模块\n",
    "\n",
    "#读取文件路径为入参的csv文件。返回DataFram类型的 文件内容\n",
    "def mLoad_Data_Fun(mFileNamePara_str:str)->pd.DataFrame:\n",
    "    mOrgData_pd = pd.read_csv(mFileNamePara_str)#'D:\\\\mMLP\\\\StockData\\\\mdata0.csv')\n",
    "    return mOrgData_pd\n",
    "\n",
    "\n",
    "#数据预处理   高、开、低、收,成交量 的数据有意义的是其相对于昨日的涨跌幅\n",
    "def mPreDeal_OneData_Fun(mParaPrice_array:np.array)->np.array:\n",
    "    mDataTemp_array = []\n",
    "    i = 1\n",
    "    while(i<len(mParaPrice_array)):\n",
    "        #因为最大涨跌幅是10%，所以*10 将数据限制在-1-1间\n",
    "        mData_D = (mParaPrice_array[i] / mParaPrice_array[i-1] - 1) * 10\n",
    "        mDataTemp_array.append(mData_D)\n",
    "        i += 1\n",
    "    mDataTemp_array = np.array(mDataTemp_array)\n",
    "    return mDataTemp_array\n",
    "\n",
    "\n",
    "#数据预处理  高-收、收-开、开-低和昨日收盘价格 的比率=\n",
    "def mUpDownShiti_Percent_Fun(mHighPara_array:np.array,mOpenPara_array:np.array,mLowPara_array:np.array,\\\n",
    "                             mClosePara_array:np.array,):\n",
    "    mUpPercentTemp_np = []\n",
    "    mDownPercentTemp_np = []\n",
    "    mShitiPercentTemp_np = []\n",
    "    \n",
    "    i = 1\n",
    "    while(i < len(mClosePara_array)):\n",
    "        \n",
    "        #因为 这3个值的最大值为 20%(涨跌幅10%，天地板时)，所以* 5 将其限制在 0-1之间\n",
    "        if mOpenPara_array[i] > mClosePara_array[i]:\n",
    "            #上影线\n",
    "            mUpPercentTemp_d = (mHighPara_array[i] - mOpenPara_array[i]) / mClosePara_array[i-1] * 5\n",
    "            #下影线\n",
    "            mDownPercentTemp_d = (mClosePara_array[i] - mLowPara_array[i]) / mClosePara_array[i-1] * 5\n",
    "        else:\n",
    "            #上影线\n",
    "            mUpPercentTemp_d = (mHighPara_array[i] - mClosePara_array[i]) / mClosePara_array[i-1] * 5\n",
    "            #下影线\n",
    "            mDownPercentTemp_d = (mOpenPara_array[i] - mLowPara_array[i]) / mClosePara_array[i-1] * 5\n",
    "            \n",
    "        #实体 因为 开、收的大小关系 本身也含有信息，所以不取其绝对值\n",
    "        mShitiPercentTemp_d = (mOpenPara_array[i] - mClosePara_array[i]) / mClosePara_array[i-1] * 5\n",
    "            \n",
    "        mUpPercentTemp_np.append(mUpPercentTemp_d)\n",
    "        mDownPercentTemp_np.append(mDownPercentTemp_d)\n",
    "        mShitiPercentTemp_np.append(mShitiPercentTemp_d)\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    mUpPercentTemp_np = np.array(mUpPercentTemp_np)\n",
    "    mDownPercentTemp_np = np.array(mDownPercentTemp_np)\n",
    "    mShitiPercentTemp_np = np.array(mShitiPercentTemp_np)\n",
    "    return mUpPercentTemp_np,mDownPercentTemp_np,mShitiPercentTemp_np\n",
    "\n",
    "\n",
    "\n",
    "#判断类别y的预处理 \n",
    "def mPreDeal_Category_Y_Fun(mClosePara_array:np.array)->np.array:\n",
    "    \"\"\"\n",
    "    按 往后5天内，close的涨跌幅划分区间\n",
    "    0：跌幅大于3个点的 \n",
    "    1：跌幅在 1.5 - 0 之间的\n",
    "    2：涨幅在 0 - 1.5之间的\n",
    "    3：涨幅在 1.5 -3 之间的\n",
    "    4： 涨幅大于 3个点的\n",
    "    \n",
    "    \"\"\"\n",
    "    mReturn_array = []\n",
    "    i = 1\n",
    "    while(i<len(mClosePara_array)):\n",
    "        #如果后续天数不足5天，则取后续剩余天数为判断天数\n",
    "        if(i+4 < len(mClosePara_array)):\n",
    "            mCloseTemp_np = mClosePara_array[i:i+5]\n",
    "        else:\n",
    "            mCloseTemp_np = mClosePara_array[i:]\n",
    "        \n",
    "        mMaxClose_f = np.max(mCloseTemp_np)\n",
    "        #将张地方转为百分点新式，即3 表示张 3个百分点\n",
    "        mRate = ( mMaxClose_f / mClosePara_array[i-1]  -1 ) * 100 \n",
    "        \n",
    "        if mRate > 3:\n",
    "            mReturn_array.append(4)\n",
    "        elif mRate > 1.5:\n",
    "            mReturn_array.append(3)\n",
    "        elif mRate > 0:\n",
    "            mReturn_array.append(2)\n",
    "        elif mRate > -1.5:\n",
    "            mReturn_array.append(1)\n",
    "        else:\n",
    "            mReturn_array.append(0)\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    mReturn_array = np.array(mReturn_array)\n",
    "    \n",
    "    return mReturn_array\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eec559d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据总预处理-将数据处理成\n",
    "def mPreDeal_Total_Fun(mData_pd:pd.DataFrame):\n",
    "    mCloseTemp_array = np.array(mData_pd.loc[:,'close'])\n",
    "    mOpenTemp_array = np.array(mData_pd.loc[:,'open'])\n",
    "    mHighTemp_array = np.array(mData_pd.loc[:,'high'])\n",
    "    mLowTemp_array = np.array(mData_pd.loc[:,'low'])\n",
    "    mVolumeTemp_array = np.array(mData_pd.loc[:,'volume'])\n",
    "    \n",
    "    #忘了RSI指标的意义，暂时先搁置\n",
    "    #mRSI_array = np.array(mData_pd.loc[1:,'RSI'])\n",
    "    \n",
    "    #对高、开、低、收、成交量 进行预处理，获取其相对于其昨日数据的比率\n",
    "    mPreCloseSimple_array = mPreDeal_OneData_Fun(mCloseTemp_array)\n",
    "    mPreOpenSimple_array = mPreDeal_OneData_Fun(mOpenTemp_array)\n",
    "    mPreHighSimple_array = mPreDeal_OneData_Fun(mHighTemp_array)\n",
    "    mPreLowSimple_array = mPreDeal_OneData_Fun(mLowTemp_array)\n",
    "    mPreVolume_array = mPreDeal_OneData_Fun(mVolumeTemp_array)\n",
    "    \n",
    "    #获取上下影线、实体相对于昨日收盘价的比率\n",
    "    mUpPercent_np,mDownPercent_np,mShitiPercent_np = mUpDownShiti_Percent_Fun(mHighTemp_array,mOpenTemp_array,mLowTemp_array,\\\n",
    "                                                                              mCloseTemp_array)\n",
    "  \n",
    "    \n",
    "    mPreFinalY_list = mPreDeal_Category_Y_Fun(mCloseTemp_array)\n",
    "    \n",
    "    #7:mRSI_array,\n",
    "    mPreFinalData_pd = {0:mPreCloseSimple_array, 1:mPreOpenSimple_array, 2:mPreHighSimple_array , 3:mPreLowSimple_array \\\n",
    "                    , 4:mPreVolume_array ,5:mUpPercent_np,6:mDownPercent_np,7:mShitiPercent_np,'y':mPreFinalY_list}\n",
    "    mPreFinalData_pd = pd.DataFrame(mPreFinalData_pd)\n",
    "    \n",
    "    return mPreFinalData_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a145a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmOrgData_df = mLoad_Data_Fun('D:\\\\mMLP\\\\StockData\\\\mTrainStock\\\\600056.csv')\\nmPreData_df = mPreDeal_Total_Fun(mOrgData_df)\\nmPreData_df.head()\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "mOrgData_df = mLoad_Data_Fun('D:\\\\mMLP\\\\StockData\\\\mTrainStock\\\\600056.csv')\n",
    "mPreData_df = mPreDeal_Total_Fun(mOrgData_df)\n",
    "mPreData_df.head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a15f49c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67ba2c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据预处理 -将mPreDeal_Total_Fun预处理后的数据整理成RNN模型的输入类型\n",
    "def mPre_Change_RNNPara_Fun(mPreFinalPara_pd:pd.DataFrame, mStepPara_I=10):\n",
    "    #\n",
    "    x = []\n",
    "    y = []\n",
    "    mRangeLen_I = len(mPreFinalPara_pd)-mStepPara_I \n",
    "    for i in range(mRangeLen_I):\n",
    "        mDataTemp = np.array(mPreFinalPara_pd.loc[i:i+mStepPara_I-1,:].drop('y',axis=1)) \n",
    "        x.append(mDataTemp) \n",
    "        y.append(mPreFinalPara_pd.loc[i+mStepPara_I-1,'y']) \n",
    "        \n",
    "\n",
    "    x = np.array(x)\n",
    "    y = pd.Series(y)\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6467edba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmPreDataRnn_x,mPreDataRnn_y = mPre_Change_RNNPara_Fun(mPreData_df,20)\\nprint(mPreDataRnn_x.shape,mPreDataRnn_y.shape)\\nprint(mPreDataRnn_x[1])\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "mPreDataRnn_x,mPreDataRnn_y = mPre_Change_RNNPara_Fun(mPreData_df,20)\n",
    "print(mPreDataRnn_x.shape,mPreDataRnn_y.shape)\n",
    "print(mPreDataRnn_x[1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4280835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#转化为one-hot类型\n",
    "def mYChange_One_hot_Fun(mYTrainPara_list:pd.Series):\n",
    "    y_train = np_utils.to_categorical(mYTrainPara_list, 5)\n",
    "    return y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42dd50fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#将Y转为one-hot向量\\nmPreDataRnnOne_y = mYChange_One_hot_Fun(mPreDataRnn_y)\\n\\n#训练、测试集分离\\nmXTrain_np = mPreDataRnn_x[:-800]\\nmYTrain = mPreDataRnnOne_y[:-800]\\n\\nmXTest_np = mPreDataRnn_x[-800:-500]\\nmYTest_np = mPreDataRnnOne_y[-800:-500]\\n\\nmXShape = mXTrain_np[0].shape\\nmYshape = mYTrain[0].shape\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#将Y转为one-hot向量\n",
    "mPreDataRnnOne_y = mYChange_One_hot_Fun(mPreDataRnn_y)\n",
    "\n",
    "#训练、测试集分离\n",
    "mXTrain_np = mPreDataRnn_x[:-800]\n",
    "mYTrain = mPreDataRnnOne_y[:-800]\n",
    "\n",
    "mXTest_np = mPreDataRnn_x[-800:-500]\n",
    "mYTest_np = mPreDataRnnOne_y[-800:-500]\n",
    "\n",
    "mXShape = mXTrain_np[0].shape\n",
    "mYshape = mYTrain[0].shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99de0ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#搭建RNN模型\n",
    "#--------------------2Rnn2MLP_tanh_softmax_10_5_5_4_SGD_0.001_\n",
    "def mCreate_Rnn_Model_Fun(mRnnXDataShape,mRnnYshape):\n",
    "    #优化器选择与设置\n",
    "    #mOptimizer_obj = RMSprop(learning_rate=0.001)\n",
    "    mOptimizer_obj =Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    mRnnModel = Sequential()\n",
    "    mRnnModel.add(SimpleRNN(units = 20,input_shape=(mRnnXDataShape[0],mRnnXDataShape[1]),activation='tanh'\\\n",
    "                            ,name='mFirst_Rnn_layer'))#,return_sequences='True'\n",
    "    \n",
    "    #mRnnModel.add(SimpleRNN(units = 10,activation='sigmoid',name='mSecond_Rnn_layer'))#sigmoid\n",
    "    \n",
    "    mRnnModel.add(Dense(units=10,activation='tanh',name='mFourth_MLP_layers'))\n",
    "    \n",
    "    mRnnModel.add(Dense(units=mRnnYshape[0],activation='softmax',name='mSecond_MLP_layers'))\n",
    "    \n",
    "    mRnnModel.summary()\n",
    "    \n",
    "    \n",
    "    mRnnModel.compile(loss='categorical_crossentropy',optimizer=mOptimizer_obj,metrics=['accuracy'])\n",
    "    \n",
    "    return mRnnModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "642a0743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmRnnModel_model = mCreate_Rnn_Model_Fun(mXShape,mYshape) \\n    \\n#对分类为3 重点关注\\nmClaseWeight_dic = {0:2.0, 1:1.0 , 2: 1.0 , 3:2.0 , 4:2.0}\\n    \\nearly_stopping = EarlyStopping(monitor='accuracy', patience=20)\\n#,callbacks=[early_stopping]\\nmTrainResult_obj = mRnnModel_model.fit(mXTrain_np,mYTrain,class_weight=mClaseWeight_dic,epochs=1000,batch_size=150,                                       validation_data =(mXTest_np,mYTest_np),callbacks=[early_stopping]);\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "mRnnModel_model = mCreate_Rnn_Model_Fun(mXShape,mYshape) \n",
    "    \n",
    "#对分类为3 重点关注\n",
    "mClaseWeight_dic = {0:2.0, 1:1.0 , 2: 1.0 , 3:2.0 , 4:2.0}\n",
    "    \n",
    "early_stopping = EarlyStopping(monitor='accuracy', patience=20)\n",
    "#,callbacks=[early_stopping]\n",
    "mTrainResult_obj = mRnnModel_model.fit(mXTrain_np,mYTrain,class_weight=mClaseWeight_dic,epochs=1000,batch_size=150,\\\n",
    "                                       validation_data =(mXTest_np,mYTest_np),callbacks=[early_stopping]);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28d37713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mGet_RNN_FinalPreDate_Fun(mStockPath_str):\n",
    "    #数据读取\n",
    "    mOrgData_df = mLoad_Data_Fun(mStockPath_str)\n",
    "    #数据预处理，主要是数据的归一化\n",
    "    mPreData_df = mPreDeal_Total_Fun(mOrgData_df)\n",
    "    #数据预处理，将数据转为RNN神经网络的输入\n",
    "    mPreDataRnn_x,mPreDataRnn_y = mPre_Change_RNNPara_Fun(mPreData_df,20)\n",
    "\n",
    "    #将Y转为one-hot向量\n",
    "    mPreDataRnnOne_y = mYChange_One_hot_Fun(mPreDataRnn_y)\n",
    "\n",
    "    \n",
    "    #训练、测试集分离\n",
    "    #测试集所占比率\n",
    "    mTestRate_D = 0.2\n",
    "    mTestLocation = int(len(mPreDataRnn_x) * mTestRate_D)\n",
    "    \n",
    "    print(mTestLocation)\n",
    "    mXTrain_np = mPreDataRnn_x[:-mTestLocation]\n",
    "    mYTrain_np = mPreDataRnnOne_y[:-mTestLocation]\n",
    "\n",
    "    mXTest_np = mPreDataRnn_x[-mTestLocation:]\n",
    "    mYTest_np = mPreDataRnnOne_y[-mTestLocation:]\n",
    "    \n",
    "    return mXTrain_np,mYTrain_np,mXTest_np,mYTest_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e005ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#一支股票的训练'D:\\\\mMLP\\\\StockData\\\\mTrainStock\\\\600056.csv'\n",
    "def mRNN_Train_OneStock_Fun(mStockDataPath_str):\n",
    "    #数据读取\n",
    "    mXTrain_np,mYTrain_np,mXTest_np,mYTest_np = mGet_RNN_FinalPreDate_Fun(mStockDataPath_str)\n",
    "\n",
    "    mXShape = mXTrain_np[0].shape\n",
    "    mYshape = mYTrain_np[0].shape\n",
    "\n",
    "\n",
    "    mRnnModel_model = mCreate_Rnn_Model_Fun(mXShape,mYshape) \n",
    "\n",
    "    #对分类为3 重点关注\n",
    "    mClaseWeight_dic = {0:2.0, 1:1.0 , 2: 1.0 , 3:2.0 , 4:2.0}\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='accuracy', patience=10)\n",
    "    #,callbacks=[early_stopping],class_weight=mClaseWeight_dic,\n",
    "    mTrainResult_obj = mRnnModel_model.fit(mXTrain_np,mYTrain_np,epochs=1000,batch_size=100,\\\n",
    "                                       validation_data =(mXTest_np,mYTest_np),callbacks=[early_stopping]);\n",
    "    mAccuray = mTrainResult_obj.history['val_accuracy']\n",
    "    if mAccuray[-1] > 0.5:\n",
    "        mSymbol_str = mStockDataPath_str[-10:-4]\n",
    "        print(mSymbol_str,' accuracy ',mAccuray[-1])\n",
    "        mNeedWriteContent_str = mSymbol_str + ' accuracy ' + str(mAccuray[-1]) + '\\n'\n",
    "        mRecordFile_str = 'D:\\\\mMLP\\\\StockData\\\\mResult\\\\mSatisfy.txt'\n",
    "        \n",
    "        with open(mRecordFile_str,'a+') as mSave_obj:\n",
    "            mSave_obj.write(mNeedWriteContent_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca55cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mRnn_Train_AllStock_Fun():\n",
    "    mFilePath_str = 'E:\\\\mMachineLerning\\\\Own\\\\RNN\\\\mTrain_StockData\\\\'\n",
    "    mSymbolList_np = os.listdir(mFilePath_str)\n",
    "    \n",
    "    i = 0\n",
    "    while(i <len(mSymbolList_np)):\n",
    "        mDataPath_str = 'E:\\\\mMachineLerning\\\\Own\\\\RNN\\\\mTrain_StockData\\\\' + mSymbolList_np[i][:6] + '.csv'\n",
    "        print(i,'   ',mDataPath_str)\n",
    "        mRNN_Train_OneStock_Fun(mDataPath_str)\n",
    "        if i > 300:\n",
    "            return\n",
    "        i += 1\n",
    "    #mRNN_Train_OneStock_Fun('D:\\\\mMLP\\\\StockData\\\\mTrainStock\\\\600820.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f56d27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     E:\\mMachineLerning\\Own\\RNN\\mTrain_StockData\\000012.csv\n",
      "295\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mFirst_Rnn_layer (SimpleRNN) (None, 20)                580       \n",
      "_________________________________________________________________\n",
      "mFourth_MLP_layers (Dense)   (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "mSecond_MLP_layers (Dense)   (None, 5)                 55        \n",
      "=================================================================\n",
      "Total params: 845\n",
      "Trainable params: 845\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\csdn_ml_env\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "12/12 [==============================] - 14s 50ms/step - loss: 1.7474 - accuracy: 0.2448 - val_loss: 1.7036 - val_accuracy: 0.2712\n",
      "Epoch 2/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.7066 - accuracy: 0.2139 - val_loss: 1.6359 - val_accuracy: 0.2847\n",
      "Epoch 3/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.6414 - accuracy: 0.2165 - val_loss: 1.5887 - val_accuracy: 0.2475\n",
      "Epoch 4/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.5810 - accuracy: 0.2636 - val_loss: 1.5519 - val_accuracy: 0.2441\n",
      "Epoch 5/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.5604 - accuracy: 0.2710 - val_loss: 1.5207 - val_accuracy: 0.2915\n",
      "Epoch 6/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.5253 - accuracy: 0.3166 - val_loss: 1.4941 - val_accuracy: 0.3356\n",
      "Epoch 7/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.5131 - accuracy: 0.3152 - val_loss: 1.4674 - val_accuracy: 0.3729\n",
      "Epoch 8/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.4782 - accuracy: 0.3551 - val_loss: 1.4444 - val_accuracy: 0.3966\n",
      "Epoch 9/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.4601 - accuracy: 0.3558 - val_loss: 1.4206 - val_accuracy: 0.4000\n",
      "Epoch 10/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.4454 - accuracy: 0.3603 - val_loss: 1.3968 - val_accuracy: 0.4339\n",
      "Epoch 11/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.4334 - accuracy: 0.3520 - val_loss: 1.3783 - val_accuracy: 0.4610\n",
      "Epoch 12/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.4393 - accuracy: 0.3513 - val_loss: 1.3609 - val_accuracy: 0.4508\n",
      "Epoch 13/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.4085 - accuracy: 0.3744 - val_loss: 1.3431 - val_accuracy: 0.4576\n",
      "Epoch 14/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.3978 - accuracy: 0.3832 - val_loss: 1.3277 - val_accuracy: 0.4678\n",
      "Epoch 15/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.3795 - accuracy: 0.3788 - val_loss: 1.3180 - val_accuracy: 0.4746\n",
      "Epoch 16/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.3836 - accuracy: 0.3720 - val_loss: 1.3056 - val_accuracy: 0.4881\n",
      "Epoch 17/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.3791 - accuracy: 0.3781 - val_loss: 1.2958 - val_accuracy: 0.4881\n",
      "Epoch 18/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.3496 - accuracy: 0.4008 - val_loss: 1.2870 - val_accuracy: 0.4746\n",
      "Epoch 19/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.3519 - accuracy: 0.3972 - val_loss: 1.2811 - val_accuracy: 0.4814\n",
      "Epoch 20/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.3447 - accuracy: 0.4076 - val_loss: 1.2751 - val_accuracy: 0.4678\n",
      "Epoch 21/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.3320 - accuracy: 0.4019 - val_loss: 1.2659 - val_accuracy: 0.4746\n",
      "Epoch 22/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.3203 - accuracy: 0.4172 - val_loss: 1.2640 - val_accuracy: 0.4746\n",
      "Epoch 23/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.3028 - accuracy: 0.4151 - val_loss: 1.2611 - val_accuracy: 0.4678\n",
      "Epoch 24/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.2947 - accuracy: 0.4284 - val_loss: 1.2559 - val_accuracy: 0.4746\n",
      "Epoch 25/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.2844 - accuracy: 0.4395 - val_loss: 1.2523 - val_accuracy: 0.4881\n",
      "Epoch 26/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2736 - accuracy: 0.4460 - val_loss: 1.2538 - val_accuracy: 0.4780\n",
      "Epoch 27/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2804 - accuracy: 0.4409 - val_loss: 1.2440 - val_accuracy: 0.4881\n",
      "Epoch 28/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2623 - accuracy: 0.4626 - val_loss: 1.2493 - val_accuracy: 0.4780\n",
      "Epoch 29/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2817 - accuracy: 0.4406 - val_loss: 1.2435 - val_accuracy: 0.4814\n",
      "Epoch 30/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.2469 - accuracy: 0.4777 - val_loss: 1.2366 - val_accuracy: 0.4814\n",
      "Epoch 31/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.2790 - accuracy: 0.4405 - val_loss: 1.2384 - val_accuracy: 0.4881\n",
      "Epoch 32/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.2638 - accuracy: 0.4368 - val_loss: 1.2380 - val_accuracy: 0.4712\n",
      "Epoch 33/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.2709 - accuracy: 0.4355 - val_loss: 1.2367 - val_accuracy: 0.4712\n",
      "Epoch 34/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2410 - accuracy: 0.4531 - val_loss: 1.2358 - val_accuracy: 0.4780\n",
      "Epoch 35/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2417 - accuracy: 0.4562 - val_loss: 1.2308 - val_accuracy: 0.4847\n",
      "Epoch 36/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2429 - accuracy: 0.4569 - val_loss: 1.2336 - val_accuracy: 0.4847\n",
      "Epoch 37/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.2192 - accuracy: 0.4729 - val_loss: 1.2358 - val_accuracy: 0.4746\n",
      "Epoch 38/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.2218 - accuracy: 0.4788 - val_loss: 1.2268 - val_accuracy: 0.4915\n",
      "Epoch 39/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2264 - accuracy: 0.4734 - val_loss: 1.2285 - val_accuracy: 0.4847\n",
      "Epoch 40/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1946 - accuracy: 0.5033 - val_loss: 1.2302 - val_accuracy: 0.4780\n",
      "Epoch 41/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2343 - accuracy: 0.4660 - val_loss: 1.2210 - val_accuracy: 0.4881\n",
      "Epoch 42/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.2257 - accuracy: 0.4571 - val_loss: 1.2263 - val_accuracy: 0.4915\n",
      "Epoch 43/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.2087 - accuracy: 0.4709 - val_loss: 1.2282 - val_accuracy: 0.5051\n",
      "Epoch 44/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2187 - accuracy: 0.4653 - val_loss: 1.2240 - val_accuracy: 0.4949\n",
      "Epoch 45/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2268 - accuracy: 0.4741 - val_loss: 1.2262 - val_accuracy: 0.4949\n",
      "Epoch 46/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1952 - accuracy: 0.4891 - val_loss: 1.2194 - val_accuracy: 0.4983\n",
      "Epoch 47/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2146 - accuracy: 0.4885 - val_loss: 1.2231 - val_accuracy: 0.5017\n",
      "Epoch 48/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1892 - accuracy: 0.4881 - val_loss: 1.2217 - val_accuracy: 0.4915\n",
      "Epoch 49/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2037 - accuracy: 0.4786 - val_loss: 1.2213 - val_accuracy: 0.5085\n",
      "Epoch 50/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1741 - accuracy: 0.4887 - val_loss: 1.2219 - val_accuracy: 0.5051\n",
      "Epoch 51/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.2062 - accuracy: 0.4682 - val_loss: 1.2258 - val_accuracy: 0.4881\n",
      "Epoch 52/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1800 - accuracy: 0.4771 - val_loss: 1.2223 - val_accuracy: 0.5017\n",
      "Epoch 53/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1734 - accuracy: 0.4950 - val_loss: 1.2228 - val_accuracy: 0.4949\n",
      "Epoch 54/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1461 - accuracy: 0.4935 - val_loss: 1.2260 - val_accuracy: 0.4949\n",
      "Epoch 55/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1782 - accuracy: 0.4747 - val_loss: 1.2144 - val_accuracy: 0.5085\n",
      "Epoch 56/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1694 - accuracy: 0.5038 - val_loss: 1.2267 - val_accuracy: 0.4915\n",
      "Epoch 57/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1474 - accuracy: 0.4997 - val_loss: 1.2205 - val_accuracy: 0.5153\n",
      "Epoch 58/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1903 - accuracy: 0.4814 - val_loss: 1.2176 - val_accuracy: 0.5085\n",
      "Epoch 59/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1490 - accuracy: 0.4994 - val_loss: 1.2247 - val_accuracy: 0.5085\n",
      "Epoch 60/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1435 - accuracy: 0.4997 - val_loss: 1.2195 - val_accuracy: 0.5051\n",
      "Epoch 61/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1434 - accuracy: 0.4988 - val_loss: 1.2202 - val_accuracy: 0.4949\n",
      "Epoch 62/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1209 - accuracy: 0.5129 - val_loss: 1.2238 - val_accuracy: 0.4949\n",
      "Epoch 63/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1506 - accuracy: 0.4974 - val_loss: 1.2164 - val_accuracy: 0.5051\n",
      "Epoch 64/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1564 - accuracy: 0.4935 - val_loss: 1.2222 - val_accuracy: 0.4915\n",
      "Epoch 65/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1510 - accuracy: 0.4903 - val_loss: 1.2207 - val_accuracy: 0.5017\n",
      "Epoch 66/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1250 - accuracy: 0.5084 - val_loss: 1.2288 - val_accuracy: 0.5017\n",
      "Epoch 67/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1116 - accuracy: 0.5225 - val_loss: 1.2219 - val_accuracy: 0.5051\n",
      "Epoch 68/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1348 - accuracy: 0.5135 - val_loss: 1.2237 - val_accuracy: 0.5017\n",
      "Epoch 69/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1408 - accuracy: 0.5146 - val_loss: 1.2279 - val_accuracy: 0.5119\n",
      "Epoch 70/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1419 - accuracy: 0.5018 - val_loss: 1.2232 - val_accuracy: 0.5017\n",
      "Epoch 71/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1115 - accuracy: 0.5089 - val_loss: 1.2219 - val_accuracy: 0.5017\n",
      "Epoch 72/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1067 - accuracy: 0.5249 - val_loss: 1.2244 - val_accuracy: 0.4983\n",
      "Epoch 73/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1172 - accuracy: 0.5256 - val_loss: 1.2258 - val_accuracy: 0.4983\n",
      "Epoch 74/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1189 - accuracy: 0.5043 - val_loss: 1.2216 - val_accuracy: 0.5017\n",
      "Epoch 75/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1131 - accuracy: 0.5230 - val_loss: 1.2262 - val_accuracy: 0.4949\n",
      "Epoch 76/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1165 - accuracy: 0.5200 - val_loss: 1.2166 - val_accuracy: 0.5186\n",
      "Epoch 77/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1334 - accuracy: 0.5042 - val_loss: 1.2286 - val_accuracy: 0.5051\n",
      "Epoch 78/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1136 - accuracy: 0.5245 - val_loss: 1.2184 - val_accuracy: 0.5085\n",
      "Epoch 79/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1337 - accuracy: 0.5086 - val_loss: 1.2268 - val_accuracy: 0.4949\n",
      "Epoch 80/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0813 - accuracy: 0.5368 - val_loss: 1.2273 - val_accuracy: 0.5085\n",
      "Epoch 81/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1145 - accuracy: 0.5199 - val_loss: 1.2258 - val_accuracy: 0.5085\n",
      "Epoch 82/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1166 - accuracy: 0.5347 - val_loss: 1.2321 - val_accuracy: 0.5051\n",
      "Epoch 83/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0856 - accuracy: 0.5269 - val_loss: 1.2240 - val_accuracy: 0.4983\n",
      "Epoch 84/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0944 - accuracy: 0.5316 - val_loss: 1.2353 - val_accuracy: 0.4983\n",
      "Epoch 85/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0879 - accuracy: 0.5172 - val_loss: 1.2282 - val_accuracy: 0.5051\n",
      "Epoch 86/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0957 - accuracy: 0.5239 - val_loss: 1.2392 - val_accuracy: 0.4949\n",
      "Epoch 87/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0859 - accuracy: 0.5431 - val_loss: 1.2271 - val_accuracy: 0.5017\n",
      "Epoch 88/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0810 - accuracy: 0.5370 - val_loss: 1.2317 - val_accuracy: 0.5085\n",
      "Epoch 89/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0730 - accuracy: 0.5341 - val_loss: 1.2373 - val_accuracy: 0.5017\n",
      "Epoch 90/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0887 - accuracy: 0.5434 - val_loss: 1.2242 - val_accuracy: 0.5153\n",
      "Epoch 91/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0686 - accuracy: 0.5529 - val_loss: 1.2420 - val_accuracy: 0.4949\n",
      "Epoch 92/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0617 - accuracy: 0.5419 - val_loss: 1.2358 - val_accuracy: 0.5051\n",
      "Epoch 93/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0698 - accuracy: 0.5414 - val_loss: 1.2280 - val_accuracy: 0.4983\n",
      "Epoch 94/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0830 - accuracy: 0.5311 - val_loss: 1.2407 - val_accuracy: 0.5017\n",
      "Epoch 95/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0484 - accuracy: 0.5560 - val_loss: 1.2269 - val_accuracy: 0.5220\n",
      "Epoch 96/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0679 - accuracy: 0.5335 - val_loss: 1.2571 - val_accuracy: 0.4746\n",
      "Epoch 97/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0583 - accuracy: 0.5475 - val_loss: 1.2324 - val_accuracy: 0.5119\n",
      "Epoch 98/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0647 - accuracy: 0.5492 - val_loss: 1.2593 - val_accuracy: 0.4814\n",
      "Epoch 99/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0656 - accuracy: 0.5635 - val_loss: 1.2496 - val_accuracy: 0.4915\n",
      "Epoch 100/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0159 - accuracy: 0.5669 - val_loss: 1.2347 - val_accuracy: 0.5119\n",
      "Epoch 101/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0477 - accuracy: 0.5734 - val_loss: 1.2447 - val_accuracy: 0.4949\n",
      "Epoch 102/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0538 - accuracy: 0.5471 - val_loss: 1.2474 - val_accuracy: 0.5017\n",
      "Epoch 103/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0586 - accuracy: 0.5705 - val_loss: 1.2477 - val_accuracy: 0.5051\n",
      "Epoch 104/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0248 - accuracy: 0.5687 - val_loss: 1.2495 - val_accuracy: 0.4983\n",
      "Epoch 105/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0215 - accuracy: 0.5856 - val_loss: 1.2347 - val_accuracy: 0.5119\n",
      "Epoch 106/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0532 - accuracy: 0.5699 - val_loss: 1.2449 - val_accuracy: 0.5153\n",
      "Epoch 107/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0165 - accuracy: 0.5678 - val_loss: 1.2566 - val_accuracy: 0.5017\n",
      "Epoch 108/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0245 - accuracy: 0.5837 - val_loss: 1.2518 - val_accuracy: 0.5085\n",
      "Epoch 109/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0419 - accuracy: 0.5490 - val_loss: 1.2838 - val_accuracy: 0.4881\n",
      "Epoch 110/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0066 - accuracy: 0.5860 - val_loss: 1.3089 - val_accuracy: 0.4780\n",
      "Epoch 111/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0238 - accuracy: 0.5811 - val_loss: 1.2569 - val_accuracy: 0.5085\n",
      "Epoch 112/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0360 - accuracy: 0.5482 - val_loss: 1.2433 - val_accuracy: 0.5186\n",
      "Epoch 113/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0151 - accuracy: 0.5804 - val_loss: 1.2472 - val_accuracy: 0.5186\n",
      "Epoch 114/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.9708 - accuracy: 0.6046 - val_loss: 1.3115 - val_accuracy: 0.4678\n",
      "Epoch 115/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0137 - accuracy: 0.5879 - val_loss: 1.2648 - val_accuracy: 0.4881\n",
      "Epoch 116/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9871 - accuracy: 0.5962 - val_loss: 1.2573 - val_accuracy: 0.4949\n",
      "Epoch 117/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9729 - accuracy: 0.5976 - val_loss: 1.2692 - val_accuracy: 0.4949\n",
      "Epoch 118/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.9798 - accuracy: 0.6024 - val_loss: 1.2707 - val_accuracy: 0.4949\n",
      "Epoch 119/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9669 - accuracy: 0.6052 - val_loss: 1.2601 - val_accuracy: 0.5085\n",
      "Epoch 120/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9774 - accuracy: 0.5901 - val_loss: 1.3032 - val_accuracy: 0.4746\n",
      "Epoch 121/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9512 - accuracy: 0.5975 - val_loss: 1.3012 - val_accuracy: 0.4712\n",
      "Epoch 122/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9536 - accuracy: 0.6225 - val_loss: 1.3012 - val_accuracy: 0.4915\n",
      "Epoch 123/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9623 - accuracy: 0.6055 - val_loss: 1.3084 - val_accuracy: 0.4746\n",
      "Epoch 124/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9470 - accuracy: 0.6169 - val_loss: 1.3137 - val_accuracy: 0.4712\n",
      "Epoch 125/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9490 - accuracy: 0.6160 - val_loss: 1.3025 - val_accuracy: 0.4847\n",
      "Epoch 126/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.9444 - accuracy: 0.6203 - val_loss: 1.3037 - val_accuracy: 0.4678\n",
      "Epoch 127/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.9640 - accuracy: 0.5895 - val_loss: 1.3173 - val_accuracy: 0.4814\n",
      "Epoch 128/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.9355 - accuracy: 0.6194 - val_loss: 1.3247 - val_accuracy: 0.4780\n",
      "Epoch 129/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9294 - accuracy: 0.6226 - val_loss: 1.3402 - val_accuracy: 0.4644\n",
      "Epoch 130/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9369 - accuracy: 0.6077 - val_loss: 1.3629 - val_accuracy: 0.4542\n",
      "Epoch 131/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.9742 - accuracy: 0.5938 - val_loss: 1.3411 - val_accuracy: 0.4644\n",
      "Epoch 132/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9096 - accuracy: 0.6265 - val_loss: 1.3262 - val_accuracy: 0.4712\n",
      "Epoch 133/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9703 - accuracy: 0.6035 - val_loss: 1.3483 - val_accuracy: 0.4678\n",
      "Epoch 134/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9356 - accuracy: 0.6222 - val_loss: 1.3139 - val_accuracy: 0.4746\n",
      "Epoch 135/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9246 - accuracy: 0.6341 - val_loss: 1.3047 - val_accuracy: 0.4983\n",
      "Epoch 136/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9509 - accuracy: 0.5896 - val_loss: 1.3279 - val_accuracy: 0.4712\n",
      "Epoch 137/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.9368 - accuracy: 0.6042 - val_loss: 1.3270 - val_accuracy: 0.4780\n",
      "Epoch 138/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.9098 - accuracy: 0.6238 - val_loss: 1.3273 - val_accuracy: 0.4814\n",
      "Epoch 139/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8994 - accuracy: 0.6142 - val_loss: 1.3298 - val_accuracy: 0.4881\n",
      "Epoch 140/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9142 - accuracy: 0.6191 - val_loss: 1.3323 - val_accuracy: 0.4915\n",
      "Epoch 141/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.9285 - accuracy: 0.6327 - val_loss: 1.3497 - val_accuracy: 0.4712\n",
      "Epoch 142/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8741 - accuracy: 0.6340 - val_loss: 1.3439 - val_accuracy: 0.4678\n",
      "Epoch 143/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.9132 - accuracy: 0.6307 - val_loss: 1.3609 - val_accuracy: 0.4610\n",
      "Epoch 144/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8956 - accuracy: 0.6305 - val_loss: 1.3671 - val_accuracy: 0.4508\n",
      "Epoch 145/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.9373 - accuracy: 0.6066 - val_loss: 1.3704 - val_accuracy: 0.4542\n",
      "Epoch 146/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.8953 - accuracy: 0.6341 - val_loss: 1.4015 - val_accuracy: 0.4610\n",
      "Epoch 147/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8910 - accuracy: 0.6370 - val_loss: 1.3701 - val_accuracy: 0.4712\n",
      "Epoch 148/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8853 - accuracy: 0.6140 - val_loss: 1.3760 - val_accuracy: 0.4746\n",
      "Epoch 149/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8805 - accuracy: 0.6407 - val_loss: 1.3707 - val_accuracy: 0.4644\n",
      "Epoch 150/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.8747 - accuracy: 0.6331 - val_loss: 1.3928 - val_accuracy: 0.4712\n",
      "Epoch 151/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8677 - accuracy: 0.6272 - val_loss: 1.4028 - val_accuracy: 0.4576\n",
      "Epoch 152/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.8580 - accuracy: 0.6387 - val_loss: 1.3855 - val_accuracy: 0.4746\n",
      "Epoch 153/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8719 - accuracy: 0.6359 - val_loss: 1.3894 - val_accuracy: 0.4678\n",
      "Epoch 154/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8665 - accuracy: 0.6496 - val_loss: 1.4059 - val_accuracy: 0.4746\n",
      "Epoch 155/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8564 - accuracy: 0.6681 - val_loss: 1.3977 - val_accuracy: 0.4847\n",
      "Epoch 156/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8485 - accuracy: 0.6389 - val_loss: 1.3794 - val_accuracy: 0.4949\n",
      "Epoch 157/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8688 - accuracy: 0.6459 - val_loss: 1.3861 - val_accuracy: 0.4814\n",
      "Epoch 158/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8511 - accuracy: 0.6446 - val_loss: 1.4041 - val_accuracy: 0.4881\n",
      "Epoch 159/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8212 - accuracy: 0.6614 - val_loss: 1.3946 - val_accuracy: 0.4915\n",
      "Epoch 160/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8263 - accuracy: 0.6631 - val_loss: 1.4142 - val_accuracy: 0.4746\n",
      "Epoch 161/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8555 - accuracy: 0.6549 - val_loss: 1.4208 - val_accuracy: 0.4780\n",
      "Epoch 162/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8483 - accuracy: 0.6479 - val_loss: 1.4090 - val_accuracy: 0.4814\n",
      "Epoch 163/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8129 - accuracy: 0.6810 - val_loss: 1.4041 - val_accuracy: 0.4949\n",
      "Epoch 164/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8215 - accuracy: 0.6703 - val_loss: 1.3964 - val_accuracy: 0.5051\n",
      "Epoch 165/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8565 - accuracy: 0.6601 - val_loss: 1.4360 - val_accuracy: 0.4847\n",
      "Epoch 166/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8393 - accuracy: 0.6516 - val_loss: 1.4297 - val_accuracy: 0.4712\n",
      "Epoch 167/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8432 - accuracy: 0.6490 - val_loss: 1.4378 - val_accuracy: 0.4780\n",
      "Epoch 168/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.8341 - accuracy: 0.6733 - val_loss: 1.4240 - val_accuracy: 0.4983\n",
      "Epoch 169/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8194 - accuracy: 0.6721 - val_loss: 1.4501 - val_accuracy: 0.4678\n",
      "Epoch 170/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8421 - accuracy: 0.6533 - val_loss: 1.4527 - val_accuracy: 0.4746\n",
      "Epoch 171/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7868 - accuracy: 0.6951 - val_loss: 1.4503 - val_accuracy: 0.4847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8076 - accuracy: 0.6796 - val_loss: 1.4612 - val_accuracy: 0.4712\n",
      "Epoch 173/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8273 - accuracy: 0.6729 - val_loss: 1.4674 - val_accuracy: 0.4780\n",
      "Epoch 174/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8428 - accuracy: 0.6745 - val_loss: 1.4563 - val_accuracy: 0.4780\n",
      "Epoch 175/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8232 - accuracy: 0.6698 - val_loss: 1.4400 - val_accuracy: 0.5051\n",
      "Epoch 176/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.8010 - accuracy: 0.6817 - val_loss: 1.4355 - val_accuracy: 0.5051\n",
      "Epoch 177/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7919 - accuracy: 0.6748 - val_loss: 1.4501 - val_accuracy: 0.4949\n",
      "Epoch 178/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7853 - accuracy: 0.6798 - val_loss: 1.4522 - val_accuracy: 0.4949\n",
      "Epoch 179/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.7998 - accuracy: 0.6852 - val_loss: 1.4711 - val_accuracy: 0.4847\n",
      "Epoch 180/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8014 - accuracy: 0.6709 - val_loss: 1.4574 - val_accuracy: 0.5051\n",
      "Epoch 181/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.7714 - accuracy: 0.6999 - val_loss: 1.4719 - val_accuracy: 0.4881\n",
      "Epoch 182/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7848 - accuracy: 0.7007 - val_loss: 1.4659 - val_accuracy: 0.5085\n",
      "Epoch 183/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7866 - accuracy: 0.6819 - val_loss: 1.4930 - val_accuracy: 0.4949\n",
      "Epoch 184/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7762 - accuracy: 0.6847 - val_loss: 1.4728 - val_accuracy: 0.4949\n",
      "Epoch 185/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7664 - accuracy: 0.6986 - val_loss: 1.5024 - val_accuracy: 0.4915\n",
      "Epoch 186/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7831 - accuracy: 0.6991 - val_loss: 1.4958 - val_accuracy: 0.4881\n",
      "Epoch 187/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7739 - accuracy: 0.6972 - val_loss: 1.5029 - val_accuracy: 0.4915\n",
      "Epoch 188/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7376 - accuracy: 0.6989 - val_loss: 1.5111 - val_accuracy: 0.4881\n",
      "Epoch 189/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7625 - accuracy: 0.7060 - val_loss: 1.5330 - val_accuracy: 0.4780\n",
      "Epoch 190/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7769 - accuracy: 0.6969 - val_loss: 1.4871 - val_accuracy: 0.5119\n",
      "Epoch 191/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.7704 - accuracy: 0.6884 - val_loss: 1.5085 - val_accuracy: 0.5017\n",
      "Epoch 192/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7501 - accuracy: 0.7000 - val_loss: 1.5161 - val_accuracy: 0.4915\n",
      "Epoch 193/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.7711 - accuracy: 0.6866 - val_loss: 1.5158 - val_accuracy: 0.4983\n",
      "Epoch 194/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7776 - accuracy: 0.6927 - val_loss: 1.5343 - val_accuracy: 0.4780\n",
      "Epoch 195/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7451 - accuracy: 0.6931 - val_loss: 1.5398 - val_accuracy: 0.4746\n",
      "Epoch 196/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7464 - accuracy: 0.7006 - val_loss: 1.5615 - val_accuracy: 0.4746\n",
      "Epoch 197/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7620 - accuracy: 0.6951 - val_loss: 1.5442 - val_accuracy: 0.4949\n",
      "Epoch 198/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7723 - accuracy: 0.6917 - val_loss: 1.5294 - val_accuracy: 0.4983\n",
      "Epoch 199/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7531 - accuracy: 0.6981 - val_loss: 1.5688 - val_accuracy: 0.4746\n",
      "Epoch 200/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7832 - accuracy: 0.7012 - val_loss: 1.5274 - val_accuracy: 0.4983\n",
      "Epoch 201/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7868 - accuracy: 0.7034 - val_loss: 1.5314 - val_accuracy: 0.5051\n",
      "Epoch 202/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.7839 - accuracy: 0.6706 - val_loss: 1.5463 - val_accuracy: 0.5017\n",
      "Epoch 203/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7447 - accuracy: 0.6997 - val_loss: 1.5390 - val_accuracy: 0.5085\n",
      "Epoch 204/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7350 - accuracy: 0.7101 - val_loss: 1.5413 - val_accuracy: 0.5119\n",
      "Epoch 205/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7611 - accuracy: 0.6905 - val_loss: 1.5445 - val_accuracy: 0.5153\n",
      "Epoch 206/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7419 - accuracy: 0.7086 - val_loss: 1.5513 - val_accuracy: 0.5220\n",
      "Epoch 207/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7434 - accuracy: 0.7072 - val_loss: 1.5566 - val_accuracy: 0.5119\n",
      "Epoch 208/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7251 - accuracy: 0.7111 - val_loss: 1.5393 - val_accuracy: 0.5288\n",
      "Epoch 209/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7445 - accuracy: 0.6998 - val_loss: 1.5484 - val_accuracy: 0.5254\n",
      "Epoch 210/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7622 - accuracy: 0.6755 - val_loss: 1.5584 - val_accuracy: 0.5119\n",
      "Epoch 211/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7452 - accuracy: 0.7020 - val_loss: 1.5564 - val_accuracy: 0.5085\n",
      "Epoch 212/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7149 - accuracy: 0.7146 - val_loss: 1.5776 - val_accuracy: 0.5119\n",
      "Epoch 213/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7129 - accuracy: 0.7155 - val_loss: 1.5636 - val_accuracy: 0.5186\n",
      "Epoch 214/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7326 - accuracy: 0.7055 - val_loss: 1.5835 - val_accuracy: 0.5085\n",
      "Epoch 215/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7407 - accuracy: 0.7108 - val_loss: 1.5780 - val_accuracy: 0.4983\n",
      "Epoch 216/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7307 - accuracy: 0.7106 - val_loss: 1.5960 - val_accuracy: 0.5119\n",
      "Epoch 217/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6982 - accuracy: 0.7243 - val_loss: 1.5900 - val_accuracy: 0.5085\n",
      "Epoch 218/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7206 - accuracy: 0.7074 - val_loss: 1.6104 - val_accuracy: 0.4949\n",
      "Epoch 219/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7335 - accuracy: 0.7102 - val_loss: 1.6010 - val_accuracy: 0.5085\n",
      "Epoch 220/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.7329 - val_loss: 1.6124 - val_accuracy: 0.4915\n",
      "Epoch 221/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7156 - accuracy: 0.7060 - val_loss: 1.6497 - val_accuracy: 0.4881\n",
      "Epoch 222/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7070 - accuracy: 0.7219 - val_loss: 1.6243 - val_accuracy: 0.4949\n",
      "Epoch 223/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7068 - accuracy: 0.7292 - val_loss: 1.6330 - val_accuracy: 0.4881\n",
      "Epoch 224/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.7007 - accuracy: 0.7294 - val_loss: 1.6411 - val_accuracy: 0.4712\n",
      "Epoch 225/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7641 - accuracy: 0.7077 - val_loss: 1.6414 - val_accuracy: 0.4983\n",
      "Epoch 226/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7153 - accuracy: 0.7225 - val_loss: 1.6110 - val_accuracy: 0.5017\n",
      "Epoch 227/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6995 - accuracy: 0.7300 - val_loss: 1.6309 - val_accuracy: 0.5017\n",
      "Epoch 228/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7121 - accuracy: 0.7106 - val_loss: 1.6296 - val_accuracy: 0.5085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6741 - accuracy: 0.7366 - val_loss: 1.6412 - val_accuracy: 0.5017\n",
      "Epoch 230/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7045 - accuracy: 0.7078 - val_loss: 1.6426 - val_accuracy: 0.5051\n",
      "Epoch 231/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7048 - accuracy: 0.7160 - val_loss: 1.6454 - val_accuracy: 0.5017\n",
      "Epoch 232/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6992 - accuracy: 0.7284 - val_loss: 1.6572 - val_accuracy: 0.5017\n",
      "Epoch 233/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7198 - accuracy: 0.7244 - val_loss: 1.6639 - val_accuracy: 0.4983\n",
      "Epoch 234/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6959 - accuracy: 0.7209 - val_loss: 1.6788 - val_accuracy: 0.4983\n",
      "Epoch 235/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6843 - accuracy: 0.7302 - val_loss: 1.6561 - val_accuracy: 0.5051\n",
      "Epoch 236/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.7337 - val_loss: 1.6609 - val_accuracy: 0.5017\n",
      "Epoch 237/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6789 - accuracy: 0.7207 - val_loss: 1.6618 - val_accuracy: 0.5051\n",
      "Epoch 238/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7049 - accuracy: 0.7384 - val_loss: 1.6765 - val_accuracy: 0.5051\n",
      "Epoch 239/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.7161 - accuracy: 0.7121 - val_loss: 1.6979 - val_accuracy: 0.4915\n",
      "Epoch 240/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6811 - accuracy: 0.7385 - val_loss: 1.6914 - val_accuracy: 0.4983\n",
      "Epoch 241/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6948 - accuracy: 0.7218 - val_loss: 1.6818 - val_accuracy: 0.4949\n",
      "Epoch 242/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6922 - accuracy: 0.7328 - val_loss: 1.6859 - val_accuracy: 0.5017\n",
      "Epoch 243/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6922 - accuracy: 0.7156 - val_loss: 1.6815 - val_accuracy: 0.5051\n",
      "Epoch 244/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.6802 - accuracy: 0.7356 - val_loss: 1.6709 - val_accuracy: 0.5356\n",
      "Epoch 245/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7125 - accuracy: 0.7273 - val_loss: 1.6944 - val_accuracy: 0.5119\n",
      "Epoch 246/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6790 - accuracy: 0.7260 - val_loss: 1.6965 - val_accuracy: 0.5051\n",
      "Epoch 247/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6748 - accuracy: 0.7371 - val_loss: 1.7124 - val_accuracy: 0.5051\n",
      "Epoch 248/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6458 - accuracy: 0.7537 - val_loss: 1.6923 - val_accuracy: 0.5186\n",
      "Epoch 249/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6513 - accuracy: 0.7414 - val_loss: 1.7098 - val_accuracy: 0.5051\n",
      "Epoch 250/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6739 - accuracy: 0.7336 - val_loss: 1.7019 - val_accuracy: 0.5085\n",
      "Epoch 251/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6816 - accuracy: 0.7351 - val_loss: 1.7258 - val_accuracy: 0.5051\n",
      "Epoch 252/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6763 - accuracy: 0.7454 - val_loss: 1.7411 - val_accuracy: 0.4983\n",
      "Epoch 253/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.6483 - accuracy: 0.7452 - val_loss: 1.7248 - val_accuracy: 0.4949\n",
      "Epoch 254/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.6680 - accuracy: 0.7520 - val_loss: 1.7325 - val_accuracy: 0.5051\n",
      "Epoch 255/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6644 - accuracy: 0.7468 - val_loss: 1.7268 - val_accuracy: 0.5085\n",
      "Epoch 256/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6407 - accuracy: 0.7542 - val_loss: 1.7245 - val_accuracy: 0.5119\n",
      "Epoch 257/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6724 - accuracy: 0.7297 - val_loss: 1.7261 - val_accuracy: 0.5085\n",
      "Epoch 258/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6583 - accuracy: 0.7414 - val_loss: 1.7376 - val_accuracy: 0.5051\n",
      "Epoch 259/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6468 - accuracy: 0.7577 - val_loss: 1.7619 - val_accuracy: 0.5051\n",
      "Epoch 260/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6209 - accuracy: 0.7596 - val_loss: 1.7415 - val_accuracy: 0.4915\n",
      "Epoch 261/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6686 - accuracy: 0.7417 - val_loss: 1.7543 - val_accuracy: 0.4949\n",
      "Epoch 262/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6294 - accuracy: 0.7610 - val_loss: 1.7422 - val_accuracy: 0.5085\n",
      "Epoch 263/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6543 - accuracy: 0.7513 - val_loss: 1.7616 - val_accuracy: 0.4983\n",
      "Epoch 264/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6426 - accuracy: 0.7423 - val_loss: 1.7495 - val_accuracy: 0.5051\n",
      "Epoch 265/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6560 - accuracy: 0.7449 - val_loss: 1.7450 - val_accuracy: 0.5085\n",
      "000012  accuracy  0.508474588394165\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\mMLP\\\\StockData\\\\mResult\\\\mSatisfy.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-0c7a065364a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmRnn_Train_AllStock_Fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-fbca8b14cdc5>\u001b[0m in \u001b[0;36mmRnn_Train_AllStock_Fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mmDataPath_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'E:\\\\mMachineLerning\\\\Own\\\\RNN\\\\mTrain_StockData\\\\'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmSymbolList_np\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'   '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmDataPath_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mmRNN_Train_OneStock_Fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmDataPath_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-8117c2e56722>\u001b[0m in \u001b[0;36mmRNN_Train_OneStock_Fun\u001b[1;34m(mStockDataPath_str)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mmRecordFile_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D:\\\\mMLP\\\\StockData\\\\mResult\\\\mSatisfy.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmRecordFile_str\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'a+'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmSave_obj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[0mmSave_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmNeedWriteContent_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\mMLP\\\\StockData\\\\mResult\\\\mSatisfy.txt'"
     ]
    }
   ],
   "source": [
    "mRnn_Train_AllStock_Fun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c7b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "原始单元格格式",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
